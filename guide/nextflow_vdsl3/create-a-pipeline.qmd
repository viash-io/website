---
title: Create a pipeline
order: 40
---
  
{{< include ../../_includes/_clone_template.qmd >}}

This guide explains how to create an example pipeline that's closer to a typical use-case of a Nextflow bioinformatics pipeline.

Please review the [VDSL3 principles section](/guide/nextflow_vdsl3/principles.html) for the necessary background.

## Get the template project

To get started with building a pipeline, we provide a [template project](https://github.com/viash-io/viash_project_template)
which already contains a few components. First create a new repository by clicking the "Use this template" button in the [viash_project_template](https://github.com/viash-io/viash_project_template) repository or clicking the button below.

[Use project template](https://github.com/viash-io/viash_project_template/generate){class="btn btn-info btn-md"}

Then clone the repository using the following command.

```bash
git clone https://github.com/youruser/my_first_pipeline.git
```

The pipeline contains three components and uses two utility components from [`vsh_utils`](https://viash-hub.com/data-intuitive/vsh_utils) with which we will build the following pipeline:

```{mermaid}
graph TD
   A(file?.tsv) --> X[expand] 
   X --file1.tsv--> B1[/remove_comments/] --> C1[/take_column/] --> Y
   X --file2.tsv--> B2[/remove_comments/] --> C2[/take_column/] --> Y
   Y[vsh_toList] --> D[/combine_columns/]
   D --> E(output)
```

* `expand` is a component to transform a Channel event containing multiple files (in this case using a glob `?`) into multiple Channel events each containing one file to operate on.
* `remove_comments` is a Bash script which removes all lines starting with a `#` from a file. 
* `take_column` is a Python script which extracts one of the columns in a TSV file. 
* `vsh_toList` is a component/module that does the oposite as `expand`: turn multiple Channel items into one Channel item containing a list.
* `combine_columns` is an R script which combines multiple files into a TSV.

## Build the VDSL3 modules and workflow

First, we need to build the components into VDSL3 modules. Since Viash version 0.8.x this includes the workflows and subworkflows themselves as well since they are (or better /can/ be) stored under `src` and built to `target/`.

```{bash viash-ns-build}
# This line is a workaround for a known issue with Viash 0.8.2+
mkdir -p target/dependencies/vsh/data-intuitive/vsh_utils/v0.1.0
viash ns build --setup cachedbuild --parallel
```

For more information about the `--setup` and `--parallel` arguments, please refer to [the reference section](/reference/cli/ns_build.html).

The output of `viash ns build` tells us that

1. two dependencies are fetched (from [Viash Hub](https://viash-hub.com))
2. the locally defined components are built into Nextflow modules
3. the locally defined worfklow `demo_pipeline` is built (see later)
4. containers are built for the local modules

Once `viash ns build` is finished, a new **target** directory has been created containing the executables and modules grouped per platform:

```{bash}
tree target
```

## Import a VDSL3 module

### Viash version 0.8 and beyond

:::{.callout-note}
This functionality is available since Viash version 0.8.x and assumes the workflow code is encoded as a Viash component with a corresponding `config.vsh.yaml` config file.
:::

In order to use a module or subworkflow one simply has to add the module (either local or remote) to the `dependencies` slot in the Viash config file, for example:

```yaml
functionality:
  dependencies: 
    - name: demo/combine_columns
      repository: local

  repositories:
    - name: local
      type: local
```

After that, the module will be `include`d automatically during the Viash build stage. For more information, please refer to [the reference](/reference/config/functionality/dependencies/).

### All Viash versions

As illustrated by the `tree` output above, a module can be included by pointing to its location. This approach can be used for any Nextflow module (that exposes a compatible API):

```groovy
include { remove_comments } from "./target/nextflow/demo/remove_comments/main.nf"
```

## Create a pipeline

### All Viash versions

We can use a module in a conventional Nextflow pipeline which takes two input files (`file1` and `file2`) and removes the lines that contain comments (lines starting with `#`) from those files:

```{bash, include=FALSE}
cat > main.nf << 'HERE'
include { remove_comments } from "./target/nextflow/demo/remove_comments/main.nf"

workflow {

  // Create a channel with two events
  // Each event contains a string (an identifier) and a file (input)
  Channel.fromList([
      ["file1", [ input: file("resources_test/file1.tsv") ] ],
      ["file2", [ input: file("resources_test/file2.tsv") ] ]
    ])

    // View channel contents
    | view { tup -> "Input: $tup" }
    
    // Process the input file using the 'remove_comments' module.
    // This removes comment lines from the input TSV.
    | remove_comments.run(
      directives: [
        publishDir: "output/"
      ]
    )

    // View channel contents
    | view { tup -> "Output: $tup" }
}

HERE
```

```{embed, lang="groovy"}
main.nf
```

In plain English, the workflow works as follows:

1. Create a Channel with 2 items, corresponding to 2 input files.
2. Specify the respective input files as corresponding to the `--input` argument: `[ input: ... ]`.
3. Add a `view` operation for introspection of the Channel
4. Run the `remove_comments` step and publish the results to `output/`. No additional `fromState` and `toState` arguments are specified because the defaults suffice.
5. One more `view` to show the resulting processed Channel items.

We point the reader to [the VDSL3 principles section](/guide/nextflow_vdsl3/principles.html) for more information about how data flow (aka _state_) is management in a VDSL3 workflow.

### Pipeline as a component

:::{.callout-note}
This functionality is available since Viash version 0.8.x.
:::

We can do the same but this time encoding the pipeline as a Viash compoment itself:

```groovy
workflow run_wf {
  take:
    input_ch

  main:

    output_ch = 

      // Create a channel with two events
      // Each event contains a string (an identifier) and a file (input)
      Channel.fromList([
          ["file1", [ input: file("resources_test/file1.tsv") ] ],
          ["file2", [ input: file("resources_test/file2.tsv") ] ]
        ])

        // View channel contents
        | view { tup -> "Input: $tup" }
        
        // Process the input file using the 'remove_comments' module.
        // This removes comment lines from the input TSV.
        | remove_comments

        // View channel contents
        | view { tup -> "Output: $tup" }

  emit:
    output_ch
      | map{ id, state -> [ "run", state ] }
}
```

Together with a config file like this one:

```yaml
functionality:
  name: test
  namespace: workflows
  description: |
    An example pipeline and project template.

  arguments:
    - name: "--output"
      alternatives: [ "-o" ]
      type: file
      direction: output
      required: true
      description: Output TSV file
      example: output.tsv

  resources:
    - type: nextflow_script
      path: main.nf
      entrypoint: run_wf

  dependencies: 
    - name: demo/remove_comments
      repository: local

  repositories:
    - name: local
      type: local

platforms:
  - type: nextflow
```

## Run the pipeline

Now run the pipeline with Nextflow:

```{bash}
nextflow run . \
  -main-script main.nf
```

```{bash}
tree output
```

```{bash}
cat output/*
```

## Discussion

The above example pipeline serves as the backbone for creating real-life pipelines. However, for the sake of simplicity it contained several hardcoded elements that should be avoided:

* Input parameters should be provided as an argument to the pipeline or as part of the pipeline configuration
* The output directory should be specified as an argument to the pipeline

As illustrated [earlier](#pipeline-as-a-component) these come for free when encoding the workflow as a Viash component. One even gets parameter checks with it!


```{r include=FALSE}
knitr::opts_knit$set(root.dir = normalizePath("../"))
```

```{r include=FALSE}
unlink(proj_dir, recursive = TRUE)
```

{{< include ../../_includes/_prune_all_images.qmd >}}
