---
title: Create a pipeline
order: 40
---

{{< include ../../_includes/_clone_template.qmd >}}

This guide explains how to create an example pipeline that's closer to a typical use-case of a Nextflow bioinformatics pipeline.

:::{.callout-note}
This page assumes knowledge of how to create and manipulate Nextflow channels using DSL2. For more information, check out the [Nextflow reference docs](https://www.nextflow.io/docs/latest/index.html) or contact [Data Intuitive](https://www.data-intuitive.com/contact) for a complete Nextflow+Viash course.
:::

## Get the template project

To get started with building a pipeline, we provide a [template project](https://github.com/viash-io/viash_project_template)
which already contains a few components. First create a new repository by clicking the "Use this template" button in the [viash_project_template](https://github.com/viash-io/viash_project_template) repository or clicking the button below.

[Use project template](https://github.com/viash-io/viash_project_template/generate){class="btn btn-info btn-md"}

Then clone the repository using the following command.

```bash
git clone https://github.com/youruser/my_first_pipeline.git
```

The pipeline contains three components and uses two utility components from [`vsh_utils`](https://viash-hub.com/data-intuitive/vsh_utils) with which we will build the following pipeline:

```{mermaid}
graph LR
   A(file?.tsv) --> X[expand] --> B[/remove_comments/]
   B --> C[/take_column/]
   C --> Y[vsh_toList] --> D[/combine_columns/]
   D --> E(output)
```

* `expand` is a component to transform a Channel event containing multiple files into multiple Channel events each containing one file to operate on.
* `remove_comments` is a Bash script which removes all lines starting with a `#` from a file. 
* `take_column` is a Python script which extracts one of the columns in a TSV file. 
* `vsh_toList` is a component/module that does the oposite as `expand`: turn multiple Channel items into one Channel item containing a list.
* `combine_columns` is an R script which combines multiple files into a TSV.

## Build the VDSL3 modules

First, we need to build the components into VDSL3 modules. Since Viash v0.8.x this includes the workflows and subworkflows themselves as well as they are (or better can be) stored under `src` and built to `target/`.

```{bash viash-ns-build}
viash ns build --setup cachedbuild --parallel
```

Once everything is built, a new **target** directory has been created containing the executables and modules grouped per platform:

```{bash}
tree target
```

## Importing a VDSL3 module

:::{.callout-note}
This functionality is available since Viash v0.8.x and assumes the workflow code is encoded as a Viash component with a corresponding `config.vsh.yaml` config file.
:::

In order to use a module or subworkflow one simply has to add the module (either local or remote) to the `dependencies` slot in the Viash config file, for example:

```yaml
functionality:
  dependencies: 
    - name: demo/combine_columns
      repository: local

  repositories:
    - name: local
      type: local
```

After that, the module will be `includ`ed automatically during the Viash build stage.

## VDSL3 module interface

VDSL3 modules are actually workflows which take one channel and emit one channel. It expects the channel events to be tuples containing an 'id' and a 'state':

```
[ id, state ]
```

where `id` is a unique String and `state` is a `Map[String, Object]`. The resulting channel then consists of tuples `[id, new_state]`. 

**Example:**

```groovy
workflow {
  Channel.fromList([
    ["myid", [input: file("in.txt")]]
  ])
    | mymodule
}
```

:::{.callout-note}
If the input tuple has more than two elements, the elements after the second element are passed through to the output tuple.
That is, an input tuple `[id, input, ...]` will result in a tuple `[id, output, ...]` after running the module.
For example, an input tuple `["foo", [input: file("in.txt")], "bar"]` will result in an output tuple `["foo", [output: file("out.txt")], "bar"]`.
:::

## Create a pipeline

### Conventional Nextflow pipeline

We can use a module in a conventional Nextflow pipeline which takes two input files (`file1` and `file2`) and removes the lines that contain comments:

```{bash, include=FALSE}
cat > main.nf << 'HERE'
include { remove_comments } from "./target/nextflow/demo/remove_comments/main.nf"

workflow {

  // Create a channel with two events
  // Each event contains a string (an identifier) and a file (input)
  Channel.fromList([
      ["file1", [ input: file("resources_test/file1.tsv") ] ],
      ["file2", [ input: file("resources_test/file2.tsv") ] ]
    ])

    // View channel contents
    | view { tup -> "Input: $tup" }
    
    // Process the input file using the 'remove_comments' module.
    // This removes comment lines from the input TSV.
    | remove_comments

    // View channel contents
    | view { tup -> "Output: $tup" }
}

HERE
```

### Pipeline as a component

We can do the same but this time encoding the pipeline as a Viash compoment itself:

```{embed, lang=groovy}
workflow run_wf {
  take:
    input_ch

  main:

    output_ch = 

      // Create a channel with two events
      // Each event contains a string (an identifier) and a file (input)
      Channel.fromList([
          ["file1", [ input: file("resources_test/file1.tsv") ] ],
          ["file2", [ input: file("resources_test/file2.tsv") ] ]
        ])

        // View channel contents
        | view { tup -> "Input: $tup" }
        
        // Process the input file using the 'remove_comments' module.
        // This removes comment lines from the input TSV.
        | remove_comments

        // View channel contents
        | view { tup -> "Output: $tup" }

  emit:
    output_ch
      | map{ id, state -> [ "run", state ] }
}

```

Together with a config file like this one:

```{embed, lang=yaml}
functionality:
  name: test
  namespace: workflows
  description: |
    An example pipeline and project template.

  arguments:
    - name: "--output"
      alternatives: [ "-o" ]
      type: file
      direction: output
      required: true
      description: Output TSV file
      example: output.tsv

  resources:
    - type: nextflow_script
      path: main.nf
      entrypoint: run_wf

  dependencies: 
    - name: demo/remove_comments
      repository: local

  repositories:
    - name: local
      type: local

platforms:
  - type: nextflow
```

Let's digest this script step by step.

#### Initialization

We explicitly initialize the input channel with 2 input parameters.

#### `remove_comments`

#### `view`

Note: The extra `map` is a pain. We should (IMO) have a non-strict and strict version.


<!-- TODO: refactor using the new fromState/toState args -->

## Customizing VDSL3 modules on the fly

Usually, Nextflow processes are quite static objects. For example, changing its directives can be tricky. Per-sample settings are practically impossible to achieve with vanilla Nextflow.

The `run()` function is a unique feature for every VDSL3 module which allows dynamically altering the behaviour of a module from within the pipeline. For example, we use it to set the `publishDir` directive to `"output/"` so the output of that step in the pipeline will be stored as output.

See the [reference documentation](/reference/nextflow_vdsl3/import_module.qmd#customizing-vdsl3-modules-on-the-fly) for a complete list of arguments of `.run()`.

## Run the pipeline

Now run the pipeline with Nextflow:

```{bash}
nextflow run . \
  -main-script main.nf
```

```{bash}
tree output
```

```{bash}
cat output/*
```

## Discussion

The above example pipeline serves as the backbone for creating more advanced pipelines. However, for the sake of simplicity it contained several hardcoded elements:

* Input parameters
* Output directory
* VDSL3 module directory

<!-- TODO: refactor using Viash Nxf config -->


```{r include=FALSE}
knitr::opts_knit$set(root.dir = normalizePath("../"))
```

```{r include=FALSE}
unlink(proj_dir, recursive = TRUE)
```

{{< include ../../_includes/_prune_all_images.qmd >}}
